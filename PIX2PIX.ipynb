{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "## Import TensorFlow and enable eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YfIk2es3hJEd"
   },
   "outputs": [],
   "source": [
    "# Import TensorFlow >= 1.10 and enable eager execution\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from IPython.display import clear_output\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3658248685918454822\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iYn4MdZnKCey"
   },
   "source": [
    "## Load the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2CbTEt448b4R"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 2000\n",
    "BATCH_SIZE = 1\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_dataset(Dataset_dir):\n",
    "    \n",
    "    SB_dir = os.path.join(Dataset_dir, 'images\\\\') #Satellite Bilder path\n",
    "    GT_dir = os.path.join(Dataset_dir, 'groundtruth\\\\')# Ground Truths path\n",
    "\n",
    "\n",
    "    \n",
    "    SB_listnames=glob.glob(SB_dir+\"*.png\")#Satellite Bilder filenames\n",
    "    GT_listnames=glob.glob(GT_dir+\"*.png\")# Ground Truths filenames\n",
    "    \n",
    "    GT_listnames.sort()\n",
    "    SB_listnames.sort()\n",
    "    #print(SB_listnames[0])\n",
    "    \n",
    "    print(\"Satellite Directory:\",SB_dir)\n",
    "    print('Anzahl der  Ground Truths:',len(SB_listnames))\n",
    "    print(\"\") \n",
    "    print(\"Ground Truths Directory:\",GT_dir)\n",
    "    print('Anzahl der satellien Bilder:',len(GT_listnames))\n",
    "\n",
    "    print(\"*********************************************\") \n",
    "    \n",
    "    \n",
    "    return SB_dir,GT_dir,SB_listnames,GT_listnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_Contour(image):\n",
    "\n",
    "    # Load an color image in grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "\n",
    "    \n",
    "    kernel = np.ones((2,2),np.uint8)\n",
    "    erosion = cv2.erode(gray,kernel,iterations = 2)\n",
    "\n",
    "\n",
    "    blur = cv2.GaussianBlur(erosion, (5, 5), 0)\n",
    "    (t, maskLayer) = cv2.threshold(blur, 0, 1, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "\n",
    "\n",
    "    kernel = np.ones((2,2),np.uint8)\n",
    "    dilation = cv2.dilate(maskLayer,kernel,iterations =1)\n",
    "\n",
    "\n",
    "    blur1 = cv2.GaussianBlur(dilation, (5, 5), 0)\n",
    "    (t, binary) = cv2.threshold(blur1, 0, 1,  cv2.THRESH_BINARY+ cv2.THRESH_OTSU)\n",
    "\n",
    "    (contours, hierarchy) = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) \n",
    "    #print(contours)\n",
    "    mask = np.zeros(image.shape, dtype=\"uint8\")\n",
    "    cv2.drawContours(mask, contours, -1, (0,255,0),7)\n",
    "\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Contour_overlay_GT(GT_img,Contour_array):\n",
    "    \n",
    "    #ContourOverGT = cv2.bitwise_or(GT_img, Contour_array)\n",
    "    ContourOverGT=cv2.addWeighted(GT_img,1,Contour_array,1,0)\n",
    "    #ContourOverSB = cv2.bitwise_or(GT_img, )\n",
    "\n",
    "    ContourOverGT = cv2.GaussianBlur(ContourOverGT, (5, 5), 0)\n",
    "\n",
    "    \n",
    "    return ContourOverGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(SB_path,GT_path, is_train,return_name):\n",
    "\n",
    "       \n",
    "    SB_path=str(SB_path).split(\"'\")[1]\n",
    "    SB_img =cv2.imread(SB_path)\n",
    "    SB_img= cv2.cvtColor(SB_img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    GT_path=str(GT_path).split(\"'\")[1]\n",
    "    GT_img =cv2.imread(GT_path)\n",
    "    GT_img= cv2.cvtColor(GT_img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "\n",
    "    Contour=Extract_Contour(GT_img)\n",
    "    ContourOverGT=Contour_overlay_GT(GT_img,Contour)\n",
    "\n",
    "    if is_train:        \n",
    "        if np.random.random() > 0.5:\n",
    "     # random mirroring\n",
    "            SB_img = cv2.flip( SB_img, -1);\n",
    "            ContourOverGT = cv2.flip( ContourOverGT, -1);\n",
    "\n",
    "\n",
    "    SB_img = (SB_img / 127.5) -1\n",
    "    ContourOverGT = (ContourOverGT / 127.5) - 1\n",
    "\n",
    "    SB_img=SB_img.astype(\"float32\")\n",
    "    ContourOverGT=ContourOverGT.astype(\"float32\")\n",
    "\n",
    "    if return_name:\n",
    "            return SB_img,ContourOverGT,GT_path\n",
    "        \n",
    "\n",
    "\n",
    "    return SB_img,ContourOverGT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIGN6ouoQxt3"
   },
   "source": [
    "## Use tf.data to create batches, map(do preprocessing) and shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kn-k8kTXuAlv"
   },
   "outputs": [],
   "source": [
    "Train_Dataset_dir=os.getcwd()+\"\\\\training\\\\\"\n",
    "Valid_Dataset_dir=os.getcwd()+\"\\\\training\\\\\"\n",
    "Test_Dataset_dir=os.getcwd()+\"\\\\test\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satellite Directory: C:\\Users\\Nadine\\Downloads\\training\\images\\\n",
      "Anzahl der  Ground Truths: 144\n",
      "\n",
      "Ground Truths Directory: C:\\Users\\Nadine\\Downloads\\training\\groundtruth\\\n",
      "Anzahl der satellien Bilder: 144\n",
      "*********************************************\n",
      "Satellite Directory: C:\\Users\\Nadine\\Downloads\\training\\images\\\n",
      "Anzahl der  Ground Truths: 144\n",
      "\n",
      "Ground Truths Directory: C:\\Users\\Nadine\\Downloads\\training\\groundtruth\\\n",
      "Anzahl der satellien Bilder: 144\n",
      "*********************************************\n",
      "Satellite Directory: C:\\Users\\Nadine\\Downloads\\test\\images\\\n",
      "Anzahl der  Ground Truths: 144\n",
      "\n",
      "Ground Truths Directory: C:\\Users\\Nadine\\Downloads\\test\\groundtruth\\\n",
      "Anzahl der satellien Bilder: 0\n",
      "*********************************************\n"
     ]
    }
   ],
   "source": [
    "Train_SB_dir,Train_GT_dir,SB_Train_listnames,GT_Train_listnames=Prepare_dataset(Train_Dataset_dir) #load Satellite und Ground Truths Data\n",
    "Valid_SB_dir,Valid_GT_dir,SB_Valid_listnames,GT_Valid_listnames=Prepare_dataset(Valid_Dataset_dir)\n",
    "Test_SB_dir,Test_GT_dir,SB_Test_listnames,GT_Test_listnames=Prepare_dataset(Test_Dataset_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SQHmYSmk8b4b"
   },
   "outputs": [],
   "source": [
    "#train_dataset = tf.data.Dataset.from_tensor_slices(((SB_filenames, GT_filenames)))\n",
    "SB_dataset_train = tf.data.Dataset.from_tensor_slices(SB_Train_listnames)\n",
    "GT_dataset_train = tf.data.Dataset.from_tensor_slices(GT_Train_listnames)\n",
    "train_dataset=tf.data.Dataset.zip((SB_dataset_train,GT_dataset_train))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.map(lambda x,y: tf.py_function(load_image, [x, y,True,False], [tf.float32,tf.float32]),num_parallel_calls=4)\n",
    "train_dataset = train_dataset.batch(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MS9J0yA58b4g"
   },
   "outputs": [],
   "source": [
    "SB_dataset_valid = tf.data.Dataset.from_tensor_slices(SB_Valid_listnames)\n",
    "GT_dataset_valid = tf.data.Dataset.from_tensor_slices(GT_Valid_listnames)\n",
    "valid_dataset=tf.data.Dataset.zip((SB_dataset_valid,GT_dataset_valid))\n",
    "valid_dataset = valid_dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "valid_dataset = valid_dataset.map(lambda x, y: tf.py_function(load_image, [x, y,True,False], [tf.float32,tf.float32]), num_parallel_calls=4)\n",
    "valid_dataset = valid_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SB_dataset_test = tf.data.Dataset.from_tensor_slices(SB_Test_listnames)\n",
    "GT_dataset_test = tf.data.Dataset.from_tensor_slices(GT_Test_listnames)\n",
    "test_dataset=tf.data.Dataset.zip((SB_dataset_test,GT_dataset_test))\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "test_dataset = test_dataset.map(lambda x, y: tf.py_function(load_image, [x, y,False,True], [tf.float32,tf.float32,tf.string]), num_parallel_calls=4)\n",
    "test_dataset = test_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#print(train_dataset.take(1))\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m SB, ContourOverGT \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m#print(name)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m15\u001b[39m))\n\u001b[0;32m      5\u001b[0m     display_list\u001b[38;5;241m=\u001b[39m[SB[\u001b[38;5;241m0\u001b[39m,:,:,:],ContourOverGT[\u001b[38;5;241m0\u001b[39m,:,:,:]]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#print(train_dataset.take(1))\n",
    "for SB, ContourOverGT in train_dataset.take(10):\n",
    "    #print(name)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    display_list=[SB[0,:,:,:],ContourOverGT[0,:,:,:]]\n",
    "    title = ['Input Image', 'ContourOverGT Image']\n",
    "    for i in range(2):\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(display_list[i]*.5+.5 )\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "## Write the generator and discriminator models\n",
    "\n",
    "* **Generator** \n",
    "  * The architecture of generator is a modified U-Net.\n",
    "  * Each block in the encoder is (Conv -> Batchnorm -> Leaky ReLU)\n",
    "  * Each block in the decoder is (Transposed Conv -> Batchnorm -> Dropout(applied to the first 3 blocks) -> ReLU)\n",
    "  * There are skip connections between the encoder and decoder (as in U-Net).\n",
    "  \n",
    "* **Discriminator**\n",
    "  * The Discriminator is a PatchGAN.\n",
    "  * Each block in the discriminator is (Conv -> BatchNorm -> Leaky ReLU)\n",
    "  * The shape of the output after the last layer is (batch_size, 30, 30, 1)\n",
    "  * Each 30x30 patch of the output classifies a 70x70 portion of the input image (such an architecture is called a PatchGAN).\n",
    "  * Discriminator receives 2 inputs.\n",
    "    * Input image and the target image, which it should classify as real.\n",
    "    * Input image and the generated image (output of generator), which it should classify as fake. \n",
    "    * We concatenate these 2 inputs together in the code (`tf.concat([inp, tar], axis=-1)`)\n",
    "\n",
    "* Shape of the input travelling through the generator and the discriminator is in the comments in the code.\n",
    "\n",
    "To learn more about the architecture and the hyperparameters you can refer the [paper](https://arxiv.org/abs/1611.07004).\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqqvWxlw8b4l"
   },
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lFPI4Nu-8b4q"
   },
   "outputs": [],
   "source": [
    "class Downsample(tf.keras.Model):\n",
    "    def __init__(self, filters, size, apply_batchnorm=True):\n",
    "        super(Downsample, self).__init__()\n",
    "        self.apply_batchnorm = apply_batchnorm\n",
    "        initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters, \n",
    "                                        (size, size), \n",
    "                                        strides=2, \n",
    "                                        padding='same',\n",
    "                                        kernel_initializer=initializer,\n",
    "                                        use_bias=False)\n",
    "        if self.apply_batchnorm:\n",
    "            self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "  \n",
    "    def call(self, x, training):\n",
    "        x = self.conv1(x)\n",
    "        if self.apply_batchnorm:\n",
    "            x = self.batchnorm(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "class Upsample(tf.keras.Model):\n",
    "    def __init__(self, filters, size, apply_dropout=False):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.apply_dropout = apply_dropout\n",
    "        initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "        self.up_conv = tf.keras.layers.Conv2DTranspose(filters, \n",
    "                                                   (size, size), \n",
    "                                                   strides=2, \n",
    "                                                   padding='same',\n",
    "                                                   kernel_initializer=initializer,\n",
    "                                                   use_bias=False)\n",
    "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "        if self.apply_dropout:            \n",
    "            self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "\n",
    "    def call(self, x1, x2, training):\n",
    "        x = self.up_conv(x1)\n",
    "        x = self.batchnorm(x, training=training)\n",
    "        if self.apply_dropout:\n",
    "            x = self.dropout(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.concat([x, x2], axis=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    \n",
    "        self.down1 = Downsample(64, 4, apply_batchnorm=False)\n",
    "        self.down2 = Downsample(128, 4)\n",
    "        self.down3 = Downsample(256, 4)\n",
    "        self.down4 = Downsample(512, 4)\n",
    "        self.down5 = Downsample(512, 4)\n",
    "        self.down6 = Downsample(512, 4)\n",
    "        self.down7 = Downsample(512, 4)\n",
    "        self.down8 = Downsample(512, 4)\n",
    "\n",
    "        self.up1 = Upsample(512, 4, apply_dropout=True)\n",
    "        self.up2 = Upsample(512, 4, apply_dropout=True)\n",
    "        self.up3 = Upsample(512, 4, apply_dropout=True)\n",
    "        self.up4 = Upsample(512, 4)\n",
    "        self.up5 = Upsample(256, 4)\n",
    "        self.up6 = Upsample(128, 4)\n",
    "        self.up7 = Upsample(64, 4)\n",
    "\n",
    "        self.last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, \n",
    "                                                (4, 4), \n",
    "                                                strides=2, \n",
    "                                                padding='same',\n",
    "                                                kernel_initializer=initializer)\n",
    "  \n",
    "    #@tf.compat.v1.estimator.eager.defun\n",
    "    def call(self, x, training):\n",
    "        # x shape == (bs, 256, 256, 3)    \n",
    "        x1 = self.down1(x, training=training) # (bs, 128, 128, 64)\n",
    "        x2 = self.down2(x1, training=training) # (bs, 64, 64, 128)\n",
    "        x3 = self.down3(x2, training=training) # (bs, 32, 32, 256)\n",
    "        x4 = self.down4(x3, training=training) # (bs, 16, 16, 512)\n",
    "        x5 = self.down5(x4, training=training) # (bs, 8, 8, 512)\n",
    "        x6 = self.down6(x5, training=training) # (bs, 4, 4, 512)\n",
    "        x7 = self.down7(x6, training=training) # (bs, 2, 2, 512)\n",
    "        x8 = self.down8(x7, training=training) # (bs, 1, 1, 512)\n",
    "\n",
    "        x9 = self.up1(x8, x7, training=training) # (bs, 2, 2, 1024)\n",
    "        x10 = self.up2(x9, x6, training=training) # (bs, 4, 4, 1024)\n",
    "        x11 = self.up3(x10, x5, training=training) # (bs, 8, 8, 1024)\n",
    "        x12 = self.up4(x11, x4, training=training) # (bs, 16, 16, 1024)\n",
    "        x13 = self.up5(x12, x3, training=training) # (bs, 32, 32, 512)\n",
    "        x14 = self.up6(x13, x2, training=training) # (bs, 64, 64, 256)\n",
    "        x15 = self.up7(x14, x1, training=training) # (bs, 128, 128, 128)\n",
    "\n",
    "        x16 = self.last(x15) # (bs, 256, 256, 3)\n",
    "        x16 = tf.nn.tanh(x16)\n",
    "\n",
    "        return x16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ll6aNeQx8b4v"
   },
   "outputs": [],
   "source": [
    "class DiscDownsample(tf.keras.Model):\n",
    "    def __init__(self, filters, size, apply_batchnorm=True):\n",
    "        super(DiscDownsample, self).__init__()\n",
    "        self.apply_batchnorm = apply_batchnorm\n",
    "        initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters, \n",
    "                                        (size, size), \n",
    "                                        strides=2, \n",
    "                                        padding='same',\n",
    "                                        kernel_initializer=initializer,\n",
    "                                        use_bias=False)\n",
    "        if self.apply_batchnorm:\n",
    "            self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "  \n",
    "    def call(self, x, training):\n",
    "        x = self.conv1(x)\n",
    "        if self.apply_batchnorm:\n",
    "            x = self.batchnorm(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        return x \n",
    "\n",
    "class Discriminator(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    \n",
    "        self.down1 = DiscDownsample(64, 4, False)\n",
    "        self.down2 = DiscDownsample(128, 4)\n",
    "        self.down3 = DiscDownsample(256, 4)\n",
    "    \n",
    "    # we are zero padding here with 1 because we need our shape to \n",
    "    # go from (batch_size, 32, 32, 256) to (batch_size, 31, 31, 512)\n",
    "        self.zero_pad1 = tf.keras.layers.ZeroPadding2D()\n",
    "        self.conv = tf.keras.layers.Conv2D(512, \n",
    "                                       (4, 4), \n",
    "                                       strides=1, \n",
    "                                       kernel_initializer=initializer, \n",
    "                                       use_bias=False)\n",
    "        self.batchnorm1 = tf.keras.layers.BatchNormalization()\n",
    "    \n",
    "    # shape change from (batch_size, 31, 31, 512) to (batch_size, 30, 30, 1)\n",
    "        self.zero_pad2 = tf.keras.layers.ZeroPadding2D()\n",
    "        self.last = tf.keras.layers.Conv2D(1, \n",
    "                                       (4, 4), \n",
    "                                       strides=1,\n",
    "                                       kernel_initializer=initializer)\n",
    "  \n",
    "    #@tf.compat.v1.estimator.eager.defun\n",
    "    def call(self, inp, tar, training):\n",
    "        # concatenating the input and the target\n",
    "        x = tf.concat([inp, tar], axis=-1) # (bs, 256, 256, channels*2)\n",
    "        x = self.down1(x, training=training) # (bs, 128, 128, 64)\n",
    "        x = self.down2(x, training=training) # (bs, 64, 64, 128)\n",
    "        x = self.down3(x, training=training) # (bs, 32, 32, 256)\n",
    "\n",
    "        x = self.zero_pad1(x) # (bs, 34, 34, 256)\n",
    "        x = self.conv(x)      # (bs, 31, 31, 512)\n",
    "        x = self.batchnorm1(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "    \n",
    "        x = self.zero_pad2(x) # (bs, 33, 33, 512)\n",
    "        # don't add a sigmoid activation here since\n",
    "        # the loss function expects raw logits.\n",
    "        x = self.last(x)      # (bs, cen, 30, 1 )\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gDkA05NE6QMs"
   },
   "outputs": [],
   "source": [
    "# The call function of Generator and Discriminator have been decorated\n",
    "# with tf.contrib.eager.defun()\n",
    "# We get a performance speedup if defun is used (~25 seconds per epoch)\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FMYgY_mPfTi"
   },
   "source": [
    "## Define the loss functions and the optimizer\n",
    "\n",
    "* **Discriminator loss**\n",
    "  * The discriminator loss function takes 2 inputs; **real images, generated images**\n",
    "  * real_loss is a sigmoid cross entropy loss of the **real images** and an **array of ones(since these are the real images)**\n",
    "  * generated_loss is a sigmoid cross entropy loss of the **generated images** and an **array of zeros(since these are the fake images)**\n",
    "  * Then the total_loss is the sum of real_loss and the generated_loss\n",
    "  \n",
    "* **Generator loss**\n",
    "  * It is a sigmoid cross entropy loss of the generated images and an **array of ones**.\n",
    "  * The [paper](https://arxiv.org/abs/1611.07004) also includes L1 loss which is MAE (mean absolute error) between the generated image and the target image.\n",
    "  * This allows the generated image to become structurally similar to the target image.\n",
    "  * The formula to calculate the total generator loss = gan_loss + LAMBDA * l1_loss, where LAMBDA = 100. This value was decided by the authors of the [paper](https://arxiv.org/abs/1611.07004)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cyhxTuvJyIHV"
   },
   "outputs": [],
   "source": [
    "LAMBDA = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkMNfBWlT-PV"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels = tf.ones_like(disc_real_output), \n",
    "                                              logits = disc_real_output)\n",
    "    generated_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels = tf.zeros_like(disc_generated_output), \n",
    "                                                   logits = disc_generated_output)\n",
    "\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90BIcCKcDMxz"
   },
   "outputs": [],
   "source": [
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    gan_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels = tf.ones_like(disc_generated_output),\n",
    "                                             logits = disc_generated_output) \n",
    "  # mean absolute error\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "\n",
    "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "    return total_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iWCn_PVdEJZ7"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.optimizers.Adam()\n",
    "discriminator_optimizer = tf.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aKUZnDiqQrAh"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJnftd5sQsv6"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = os.getcwd()\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"Croped_6876_200_orginalPix2Pix_C_overlay_G/\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Nadine\\\\Downloads\\\\Croped_6876_200_orginalPix2Pix_C_overlay_G/'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rw1fkAczTQYh"
   },
   "source": [
    "## Training\n",
    "\n",
    "* We start by iterating over the dataset\n",
    "* The generator gets the input image and we get a generated output.\n",
    "* The discriminator receives the input_image and the generated image as the first input. The second input is the input_image and the target_image.\n",
    "* Next, we calculate the generator and the discriminator loss.\n",
    "* Then, we calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer.\n",
    "\n",
    "## Generate Images\n",
    "\n",
    "* After training, its time to generate some images!\n",
    "* We pass images from the test dataset to the generator.\n",
    "* The generator will then translate the input image into the output we expect.\n",
    "* Last step is to plot the predictions and **voila!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NS2GWywBbAWo"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RmdVsmvhPxyy"
   },
   "outputs": [],
   "source": [
    "def generate_images(model, test_input, tar,save,address):\n",
    "\n",
    "    global name\n",
    "  # the training=True is intentional here since\n",
    "  # we want the batch statistics while running the model\n",
    "  # on the test dataset. If we use training=False, we will get \n",
    "  # the accumulated statistics learned from the training dataset\n",
    "  # (which we don't want)\n",
    "    prediction = model(test_input, training=True)\n",
    "    plt.figure(figsize=(15,15))\n",
    "\n",
    "    display_list = [test_input[0], tar[0], prediction[0]]\n",
    "    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.title(title[i])\n",
    "    # getting the pixel values between [0, 1] to plot it.\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "    if save:\n",
    "        plt.savefig(address+str(name)+'.png')\n",
    "        name +=1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2M7LmLtGEMQJ"
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "\n",
    "        for input_image, target in dataset:\n",
    "            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "                gen_output = generator(input_image, training=True)\n",
    "\n",
    "                disc_real_output = discriminator(input_image, target, training=True)\n",
    "                disc_generated_output = discriminator(input_image, gen_output, training=True)\n",
    "\n",
    "                gen_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "                disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "            generator_gradients = gen_tape.gradient(gen_loss, \n",
    "                                              generator.variables)\n",
    "            discriminator_gradients = disc_tape.gradient(disc_loss, \n",
    "                                                   discriminator.variables)\n",
    "\n",
    "            generator_optimizer.apply_gradients(zip(generator_gradients, \n",
    "                                              generator.variables))\n",
    "            discriminator_optimizer.apply_gradients(zip(discriminator_gradients, \n",
    "                                                  discriminator.variables))\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            clear_output(wait=True)\n",
    "            for inp, tar in valid_dataset.take(15):\n",
    "                generate_images(generator, inp, tar)\n",
    "          \n",
    "    # saving (checkpoint) the model every 10 epochs\n",
    "        if (epoch + 1) % 20== 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a1zZmKmvOH85",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer \"upsample_1\" (type Upsample).\n\nConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [1,8,8,512] vs. shape[1] = [1,7,7,512] [Op:ConcatV2] name: concat\n\nCall arguments received by layer \"upsample_1\" (type Upsample):\n  • x1=tf.Tensor(shape=(1, 4, 4, 1024), dtype=float32)\n  • x2=tf.Tensor(shape=(1, 7, 7, 512), dtype=float32)\n  • training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [27], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataset, epochs)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_image, target \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m gen_tape, tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m disc_tape:\n\u001b[1;32m----> 8\u001b[0m         gen_output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m         disc_real_output \u001b[38;5;241m=\u001b[39m discriminator(input_image, target, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m         disc_generated_output \u001b[38;5;241m=\u001b[39m discriminator(input_image, gen_output, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dl4cv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn [16], line 91\u001b[0m, in \u001b[0;36mGenerator.call\u001b[1;34m(self, x, training)\u001b[0m\n\u001b[0;32m     88\u001b[0m x8 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown8(x7, training\u001b[38;5;241m=\u001b[39mtraining) \u001b[38;5;66;03m# (bs, 1, 1, 512)\u001b[39;00m\n\u001b[0;32m     90\u001b[0m x9 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup1(x8, x7, training\u001b[38;5;241m=\u001b[39mtraining) \u001b[38;5;66;03m# (bs, 2, 2, 1024)\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m x10 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx9\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx6\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (bs, 4, 4, 1024)\u001b[39;00m\n\u001b[0;32m     92\u001b[0m x11 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup3(x10, x5, training\u001b[38;5;241m=\u001b[39mtraining) \u001b[38;5;66;03m# (bs, 8, 8, 1024)\u001b[39;00m\n\u001b[0;32m     93\u001b[0m x12 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup4(x11, x4, training\u001b[38;5;241m=\u001b[39mtraining) \u001b[38;5;66;03m# (bs, 16, 16, 1024)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [16], line 46\u001b[0m, in \u001b[0;36mUpsample.call\u001b[1;34m(self, x1, x2, training)\u001b[0m\n\u001b[0;32m     44\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[0;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m---> 46\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"upsample_1\" (type Upsample).\n\nConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [1,8,8,512] vs. shape[1] = [1,7,7,512] [Op:ConcatV2] name: concat\n\nCall arguments received by layer \"upsample_1\" (type Upsample):\n  • x1=tf.Tensor(shape=(1, 4, 4, 1024), dtype=float32)\n  • x2=tf.Tensor(shape=(1, 7, 7, 512), dtype=float32)\n  • training=True"
     ]
    }
   ],
   "source": [
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kz80bY3aQ1VZ"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4t4x69adQ5xb"
   },
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir 200_1500_3/  Croped_4284_200/\n",
    "checkpoint.restore(tf.train.latest_checkpoint('/media/immopixel/Amir/server/Netzwerk/GAN/master/checkpoint/Croped_6876_200_orginalPix2Pix_C_overlay_G/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1RGysMU_BZhx"
   },
   "source": [
    "## Testing on the entire test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KUgSnmy2nqSP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the trained model on the entire test dataset\n",
    "name=0\n",
    "address='/media/immopixel/Amir/server/dataset/tirol/Klein_Dataset2/test/Croped_6876_200_orginalPix2Pix_C_overlay_G_Test/'\n",
    "for inp, tar in train_dataset.take(12):\n",
    "    generate_images(generator, inp, tar,False,address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for inp, tar in train_dataset.take(10):\n",
    "    generate_images(generator, inp, tar,save=False,adress=adress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on the entire test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3AJXOByaZVOf"
   },
   "outputs": [],
   "source": [
    "def save_generated_images(model1, test_input, tar1,address,gt_name):\n",
    "    prediction1 = model1(test_input, training=True)\n",
    "    A=prediction1[0] * 0.5 + 0.5\n",
    "    # img=Image.fromarray(A)\n",
    "    mpimg.imsave(address+str(gt_name),A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the trained model on the entire test dataset\n",
    "address='/media/immopixel/Amir/server/dataset/tirol/Klein_Dataset2/test/Croped_6876_200_orginalPix2Pix_C_overlay_G_Test/'\n",
    "\n",
    "# Run the trained model on the entire test dataset\n",
    "for inp, tar1,name in test_dataset.take(774):\n",
    "    #GT_path=str(GT_path).split(\"'\")[1]\n",
    "    gt_name=str(name).split(\"'\")[1].split(\"/\")[-1]\n",
    "    #print(gt_name)\n",
    "    save_generated_images(generator,inp, tar1,address,gt_name)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pix2pix_eager.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1eb0NOTQapkYs3X0v-zL1x5_LFKgDISnp",
     "timestamp": 1527173385672
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
